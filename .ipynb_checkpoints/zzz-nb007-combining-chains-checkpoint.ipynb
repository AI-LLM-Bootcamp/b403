{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# Main Operations with LCEL Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc3c9c-e443-4a53-a3d6-98d8d23f76f3",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* Chaining runnables, coercion.\n",
    "* Multiple chains.\n",
    "* Nested chains.\n",
    "* Fallback for chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48386f20-c929-48a9-8720-0953fcd67dd0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ee060-21f2-4e01-b283-1fd656dac1e9",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 007-main-ops-lcel-chain.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edd454-6d12-445d-b4f9-b228097a1724",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a63ff6-99ff-4629-b965-547d12a99ba6",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **007-main-ops-lcel-chain**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3c1ba-c90e-4360-930b-d2ff39296e4a",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eaa23c-5e9b-4098-9aa4-9950807d1ce4",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fca6e-8eed-4ac3-abf8-e96862671a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0c516-fb73-4604-be63-c9d432411be0",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62afa07-b12c-45d4-a99e-04fe68f7f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain lanchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and start a conversation with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb756f6-05a3-41d7-9c58-ef9f02f590c1",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "* For this project, we will use OpenAI's gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b84fd-b4c6-442f-bade-e801e6c7ad81",
   "metadata": {},
   "source": [
    "## Basic chain: LLM model + prompt + output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa73509-0251-4951-9f9b-54ac1bcd73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Write one brief sentence about {politician}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f3c0a8-bc37-45da-bd60-871f6d60c7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John F. Kennedy was the 35th President of the United States, serving from 1961 until his assassination in 1963.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"politician\": \"JFK\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f35c4-015d-4a0a-8d55-ad70046c9f40",
   "metadata": {},
   "source": [
    "## Mid-level chain: Retriever App Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d52b2-3df9-40ac-bb69-cf4e06ed3a30",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aadaa7e-42e3-45c7-bacf-c3047a1fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install docarray tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a3e0d5-a939-4339-a5dd-3a050087859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliocolomer/.pyenv/versions/3.11.4/envs/venv061224-p1/lib/python3.11/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"AI Accelera has provided Generative AI Training and Consulting Services in more than 100 countries\", \"Aceleradora AI is the branch of AI Accelera for the Spanish-Speaking market\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "739774c6-5ec1-4f07-96a1-ec478d99ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "get_question_and_retrieve_relevant_docs = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e71c5-6625-4f3e-ac66-ed0648e6f3a6",
   "metadata": {},
   "source": [
    "* The previous code creates a `RunnableParallel` object with two entries.\n",
    "    * The first entry, context, will include the document results fetched by the retriever.\n",
    "    * The second entry, question, will contain the user’s original question. To pass on the question, we use `RunnablePassthrough` to copy this entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec29ebf1-9b4b-471e-b004-41ecfa0f4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Accelera has provided services in more than 100 countries.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = get_question_and_retrieve_relevant_docs | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"In how many countries has AI Accelera provided services?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43fc04-90ca-4dcf-9aaf-3e1856b94f2c",
   "metadata": {},
   "source": [
    "* Let's see a graphic representation of what this chain does:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c07e27-69e8-4e02-ad9f-9d3b738c1693",
   "metadata": {},
   "source": [
    "![alt text](./LCEL-chain-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d87c6-0e07-448a-9111-924bd03a2e2e",
   "metadata": {},
   "source": [
    "* Remember: the order of operations in a chain matters. If you try to execute the previous chain with the operations in different order, the chain will fail. We will talk more about this in a next section below.\n",
    "* In the previous exercise, this is the right order:\n",
    "    1. User input via RunnablePassthroug.\n",
    "    2. Prompt.\n",
    "    3. LLM Model.\n",
    "    4. Output Parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830adc08-3c75-4e46-9754-355e15ea2fdf",
   "metadata": {},
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f749c67b-1174-4bee-8629-9c89e7c5ead1",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbe5383-9ac3-4c3c-b722-7a536cf2425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199d6899-a0bb-4a8f-b58d-a86eba2e3455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Alumni of AI Accelera are individuals from all continents and top companies, totaling more than 7,000 trained.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 10,000 Alumni from all continents and top companies\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"who are the Alumni of AI Accelera?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675d1d4-5d30-45a0-8a2a-7e163627109b",
   "metadata": {},
   "source": [
    "#### Rememeber: the syntax of RunnableParallel can have several variations.\n",
    "* When composing a RunnableParallel with another Runnable you do not need to wrap it up in the RunnableParallel class. Inside a chain, the next three syntaxs are equivalent:\n",
    "    * `RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})`\n",
    "    * `RunnableParallel(context=retriever, question=RunnablePassthrough())`\n",
    "    * `{\"context\": retriever, \"question\": RunnablePassthrough()}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcf618-a9bc-40ce-b3ab-f970ee1bbadc",
   "metadata": {},
   "source": [
    "## Using itemgetter with RunnableParallel\n",
    "* When you are calling the LLM with several different input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b16e9cca-a8a3-4f37-bd2e-fc8a9c0c1adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arrr, AI Accelera has trained more than 3,000 Enterprise Alumni.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 3,000 Enterprise Alumni.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many Enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c333cb-4d5e-41b3-b007-8c4b1d175a93",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "* Allows you to pass inputs unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13d185f-e00d-4842-9c99-87f3d4da7a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': {'num': 1}, 'transformed_output': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    user_input = RunnablePassthrough(),\n",
    "    transformed_output = lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f207a-fdb9-46cf-b1c0-06fbcf9b5e99",
   "metadata": {},
   "source": [
    "## Chaining Runnables\n",
    "* Remember: almost any component in LangChain (prompts, models, output parsers, etc) can be used as a Runnable.\n",
    "* **Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ba28a5-a605-482a-b3df-43a70e4dc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a sentence about {politician}\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d91592b-027a-4ea4-a764-94bf27b8610f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chamberlain was a British politician who is best known for his policy of appeasement towards Nazi Germany in the years leading up to World War II.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Chamberlain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82645057-4570-4dd9-ad3b-58f0365bb4eb",
   "metadata": {},
   "source": [
    "#### Coercion: combine a chain (which is a Runnable) with other Runnables to create a new chain.\n",
    "* See how in the `composed_chain` we are including the previous `chain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8f1937-227a-4d4f-9e62-2b6792f00541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "historian_prompt = ChatPromptTemplate.from_template(\"Was {politician} positive for Humanity?\")\n",
    "\n",
    "composed_chain = {\"politician\": chain} | historian_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f98a3705-a6b0-473f-b4b8-7edd2dff9831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, Abraham Lincoln is considered to have had a positive impact on humanity. His leadership during the Civil War helped to preserve the unity of the United States and ultimately led to the end of slavery. The Emancipation Proclamation was a significant step towards equality and justice for all individuals, and Lincoln's efforts to abolish slavery have had a lasting impact on society. Additionally, Lincoln's leadership and dedication to upholding the principles of democracy and freedom have inspired generations of Americans and individuals around the world.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({\"politician\": \"Lincoln\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b650952f-a0b1-404e-ac63-1a11ed4f347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Attila the Hun's conquests and actions were not necessarily positive for humanity. He was known for his brutal tactics, including pillaging and destroying cities, and his reign brought suffering and devastation to many people. While he was certainly a powerful and formidable leader, his legacy is more often associated with violence and destruction rather than positive contributions to humanity.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({\"politician\": \"Attila\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7f6e6-7a4c-481b-b71a-445a291029ea",
   "metadata": {},
   "source": [
    "* **Functions can also be included in Runnables**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7516f36d-d166-4534-a487-a103dbd1edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_chain_with_lambda = (\n",
    "    chain\n",
    "    | (lambda input: {\"politician\": input})\n",
    "    | historian_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7431d79-fa8f-4587-9c78-aae02acc0907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It is a matter of debate whether Robespierre's actions during the Reign of Terror were ultimately positive for humanity. While he did play a key role in the French Revolution and advocated for radical social and political change, his methods were often brutal and oppressive. The Reign of Terror led to the deaths of thousands of people, many of whom were executed without fair trial. Some argue that Robespierre's actions were necessary to protect the revolution and establish a new order, while others believe that the violence and repression of the Reign of Terror ultimately did more harm than good. Ultimately, the question of whether Robespierre was a positive figure for humanity is a complex and contentious one.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain_with_lambda.invoke({\"politician\": \"Robespierre\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155f0b1-0c79-48e3-9e14-7844a1267603",
   "metadata": {},
   "source": [
    "* If you include a function in a chain, it can interfere with operations like streaming. You can learn more about this [here](https://python.langchain.com/v0.1/docs/expression_language/primitives/functions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cebf2-c24f-4bb5-bb8f-bcd2daf72f82",
   "metadata": {},
   "source": [
    "## Multiple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "172fd045-567e-40bc-8e42-e8f3923ca1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le continent où se trouve la France, dont François Mitterrand était originaire, est l'Europe.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what continent is the country {country} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"politician\": \"Miterrand\", \"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47ead9-7fb7-4fa2-951e-50026e04b43c",
   "metadata": {},
   "source": [
    "## Nested Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b788e91c-3afe-461c-9080-086c37f26a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\"\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"soccer_player\": RunnablePassthrough() \n",
    "        | RunnableLambda(russian_lastname_from_dictionary), \n",
    "        \"operation_c\": RunnablePassthrough()\n",
    "    }\n",
    ") | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc6d31c5-be72-4c95-80ab-944843450e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One curious fact about Roman Abramovich is that he is known to be a collector of rare and expensive art pieces. He has an extensive art collection that includes works by renowned artists such as Francis Bacon, Lucian Freud, and Andy Warhol. Abramovich's love for art is reflected in the design of his luxury yacht, which features a collection of expensive artworks displayed throughout the vessel.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e51df-c2fc-47d5-85d4-c4717bf32603",
   "metadata": {},
   "source": [
    "## Fallback for Chains\n",
    "* When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That's why LangChain introduced the concept of fallbacks.\n",
    "* A fallback is an alternative plan that may be used in an emergency.\n",
    "* Fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there.\n",
    "* We can create fallbacks for LCEL chains. Here we do that with two different models: ChatOpenAI (with a bad model name to easily create a chain that will error) and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509b9f90-648c-48da-acbf-206519e74966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a funny assistant who always includes a joke in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Who is the best {sport} player worldwide?\"),\n",
    "    ]\n",
    ")\n",
    "# Here we're going to use a bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model=\"gpt-fake\")\n",
    "\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d75321e-ec9c-41a3-9377-1b26d7ced7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You're a funny assistant who always includes a joke in your response.\n",
    "\n",
    "Question: Who is the best {sport} player worldwide?\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "good_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b6231c-ef6e-4d18-ba22-b92888e515cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nResponse: Well, it depends on who you ask. Some might say Messi, others might say Ronaldo. But I personally think it's my grandma, she can kick a ball farther than anyone I know!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "\n",
    "chain.invoke({\"sport\": \"soccer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99ae33-c4e7-42eb-b65e-82ce3260f9c3",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 007-main-ops-lcel-chain.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 007-main-ops-lcel-chain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688402d-d575-4339-b3e8-35da12e89e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
