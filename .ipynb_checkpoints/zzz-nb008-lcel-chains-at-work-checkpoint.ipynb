{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# LCEL chain at work in a typical RAG app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 008-lcel-chains-in-rag.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ef725-32fc-40cb-9a2b-11af72a7c4da",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **008-lcel-chains-in-rag**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d637e8-0a3c-478f-9f70-76f8d82f674f",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de05bf-4fcc-442e-88b6-0dcb6e992b6d",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e185acb-65e5-4a8d-a742-e5c2c0645d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95952fab-ebe1-44ea-8e5c-4ec07e8395f6",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428d7ca-e897-4c61-9a81-339dedcb01ce",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3f2ac-9ffb-493f-83d9-c4ba18f74559",
   "metadata": {},
   "source": [
    "## Let's see how this work with a typical RAG example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde3fa1-c1cc-47fe-a37b-014e9590756b",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following packages because they are already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "560a28d3-cb30-4d0e-8f94-20fa2c1f47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5faf7aaf-4e49-4467-836d-a802af3dc817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4831e90e-b85d-4430-a608-6fd93011774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3c150be-39db-4627-af38-9e6558d643ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865e3de-86b0-4852-bde7-53a949577fe7",
   "metadata": {},
   "source": [
    "* See below that the prompt we have imported from the hub has 2 variables: \"context\" and \"question\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "614491a8-b569-472f-8fc5-17d99256fab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef2ea3cf-17e2-4822-97fd-53b8f050e9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a task into smaller, more manageable subtasks in order to facilitate the completion of the overall task.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b035a7-4ff5-4a08-bc99-0f41d5b9d64e",
   "metadata": {},
   "source": [
    "#### Let's take a detailed look at the LCEL chain:\n",
    "* As you can see, the first part of the chain is a RunnableParallel (remember that RunnableParallel can have more than one syntax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f45961b-42d9-4e39-8a3e-92c9de0d34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    RunnableParallel({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89d1e16d-b38d-4983-94ad-97903a16939d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique that breaks down complex tasks into smaller and simpler steps. It involves prompting the model to \"think step by step\" to make tasks more manageable. This process can be done using techniques like Chain of Thought or Tree of Thoughts.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbeb5b-36de-407f-9277-5f8cda94f90a",
   "metadata": {},
   "source": [
    "* This is how this chain works when we invoke it:\n",
    "    * \"What is Task Decomposition?\" is passed as unique input.\n",
    "    * `context` executes the retriever over the input.\n",
    "    * format_docs executes the formatter function over the input.\n",
    "    * The input is assigned to `question`.\n",
    "    * the prompt is defined using the previous `question` and `context` variables.\n",
    "    * the model is executed with the previous prompt.\n",
    "    * the output parser is executed over the response of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d83a4a-8c77-47cf-a852-db2d3d7ec348",
   "metadata": {},
   "source": [
    "#### Note: what does the previos formatter function do?\n",
    "The `format_docs` function takes a list of objects named `docs`. Each object in this list is expected to have an attribute named `page_content`, which stores textual content for each document.\n",
    "\n",
    "The purpose of the function is to extract the `page_content` from each document in the `docs` list and then combine these contents into a single string. The contents of different documents are separated by two newline characters (`\\n\\n`), which means there will be an empty line between the content of each document in the final string. This formatting choice makes the combined content easier to read by clearly separating the content of different documents.\n",
    "\n",
    "Here's a breakdown of how the function works:\n",
    "1. The `for doc in docs` part iterates over each object in the `docs` list.\n",
    "2. For each iteration, `doc.page_content` accesses the `page_content` attribute of the current document, which contains its textual content.\n",
    "3. The `join` method then takes these pieces of text and concatenates them into a single string, inserting `\\n\\n` between each piece to ensure they are separated by a blank line in the final result.\n",
    "\n",
    "The function ultimately returns this newly formatted single string containing all the document contents, neatly separated by blank lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846c125-7796-497d-b1f9-cdcbac22edb2",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 008-lcel-chains-in-rag.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 008-lcel-chains-in-rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ad0cf-fbd5-45be-900e-07ca1e026220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
