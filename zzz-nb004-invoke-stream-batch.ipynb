{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Alternative ways to execute runnables\n",
    "* Invoke, Stream and Batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 004-invoke-stream-batch.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ae74d-efe9-4d20-949f-0b57df6ee289",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **004-invoke-stream-batch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662650f0-fe77-42e8-8c1a-a61aff9b72f7",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca81b4-dd76-4b4f-b714-8901d3df3e76",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c33d0-1805-4810-95ac-394fa19ce0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a04d63-0f4a-411d-94f9-decf8ed7af0e",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a004d-4c98-47ab-b720-0f09c94d42b5",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5d23-5d09-4c88-b32f-3299795cd91b",
   "metadata": {},
   "source": [
    "## LCEL Chain\n",
    "* **An LCEL Chain is a Sequence of Runnables.**\n",
    "* Almost any component in LangChain (prompts, models, output parsers, vector store retrievers, tools, etc) can be used as a Runnable.\n",
    "* Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves.\n",
    "* The order (left to right) of the elements in a LCEL chain matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a34484-0981-4edc-8ba2-7c8521d28c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "246e32f8-4380-41d2-b33a-04ff4085569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Cristiano Ronaldo is that he is known for his incredible work ethic and dedication to his fitness. He reportedly has a biological age of 23, despite being in his mid-30s, demonstrating his commitment to maintaining peak physical condition.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1026d5-2e99-4fd4-af2a-ac2c6d2eeeac",
   "metadata": {},
   "source": [
    "* All the components of the chain are Runnables.\n",
    "* **When we write chain.invoke() we are using invoke with all the componentes of the chain in an orderly manner**:\n",
    "    * First, we apply .invoke() with the user input to the prompt template.\n",
    "    * Then, with the completed prompt, we apply .invoke() to the model.\n",
    "    * And finally, we apply .invoke() to the output parser with the output of the model.\n",
    "* IMPORTANT: the order of operations in a chain matters. If you try to execute the previous chain with the components in different order, the chain will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825056a-404d-4630-b5be-77130efaf0f8",
   "metadata": {},
   "source": [
    "## Let's see this graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31565f6-f04f-4afb-91c6-598814fd0877",
   "metadata": {},
   "source": [
    "![alt text](./lcel-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dafb72-bea0-422b-93ba-d8a0ab244019",
   "metadata": {},
   "source": [
    "## And now let's replicate this process without using the LCEL chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6c802-2e5f-46e7-970b-5e336489c03e",
   "metadata": {},
   "source": [
    "#### First, we apply .invoke() with the user input to the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5fbd44-6969-4d8f-ba83-ddd9e7d72193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a curious fact about Ronaldo')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ab4a4-999d-4640-9a62-886e4fdc5ce2",
   "metadata": {},
   "source": [
    "#### Then, with the completed prompt, we apply .invoke() to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede59f87-0be9-4fda-b6c4-dd521b62f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "output_after_first_step = [HumanMessage(content='tell me a curious fact about Ronaldo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7606951-561a-4679-8f59-8fdbd097734c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ronaldo is known for his incredible work ethic and dedication to training, often spending extra hours perfecting his skills on the field. He is said to have a training routine that includes up to 3,000 sit-ups per day.', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 14, 'total_tokens': 62}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9055f0ca-b5f7-4da7-a79b-99d7e47bb00a-0', usage_metadata={'input_tokens': 14, 'output_tokens': 48, 'total_tokens': 62})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(output_after_first_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cc252-e57b-49e3-9887-7f4ccacbb4de",
   "metadata": {},
   "source": [
    "#### And finally, we apply .invoke() to the output parser with the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a157968-1b54-4cf3-bc1d-d4ee4ff2b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "output_after_second_step = AIMessage(content='One curious fact about Cristiano Ronaldo is that he does not have any tattoos on his body. Despite the fact that many professional athletes have tattoos, Ronaldo has chosen to keep his body ink-free.', response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 14, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c9812511-043a-458a-bfb8-005bc0d057fb-0', usage_metadata={'input_tokens': 14, 'output_tokens': 38, 'total_tokens': 52})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074ea557-69b2-4eed-b9ba-4344d15c35d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Cristiano Ronaldo is that he does not have any tattoos on his body. Despite the fact that many professional athletes have tattoos, Ronaldo has chosen to keep his body ink-free.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(output_after_second_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123eadeb-a912-4c8b-b42e-af1a5f848977",
   "metadata": {},
   "source": [
    "## Ways to execute Runnables\n",
    "* Remember:\n",
    "    * An LCEL Chain is a Sequence of Runnables.\n",
    "    * Almost any component in LangChain (prompts, models, output parsers, etc) can be used as a Runnable.\n",
    "    * Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves.\n",
    "    * The order (left to right) of the elements in a LCEL chain matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf49277-35e5-4061-94f0-c79270203846",
   "metadata": {},
   "source": [
    "#### LCEL Chains/Runnables are used with:\n",
    "* chain.invoke(): call the chain on an input.\n",
    "* chain.stream(): call the chain on an input and stream back chunks of the response.\n",
    "* chain.batch(): call the chain on a list of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fe651b-dc5b-4067-a430-a4fa5da3faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me one sentence about {politician}.\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd006047-876e-4d5a-a116-e4124c4324af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston Churchill was a British statesman who served as Prime Minister during World War II.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 14, 'total_tokens': 32}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8ca53718-b566-4a64-be08-677702e4e887-0', usage_metadata={'input_tokens': 14, 'output_tokens': 18, 'total_tokens': 32})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"politician\": \"Churchill\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89db19ed-7425-4506-956f-4ebb3eae44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F.D. Roosevelt was the 32nd President of the United States and served during a time of great economic hardship and world conflict."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"politician\": \"F.D. Roosevelt\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea804f6-d7f1-41aa-a6ef-ee668b8d0705",
   "metadata": {},
   "source": [
    "#### The for loop explained in simple terms\n",
    "This `for` loop is used to show responses piece by piece as they are received. Here's how it works in simple terms:\n",
    "\n",
    "1. **Getting Responses in Parts**: The loop receives pieces of a response from the system one at a time. In this example, the system is responding with information about the politician \"F.D. Roosevelt.\"\n",
    "\n",
    "2. **Printing Out Responses Immediately**: Each time a new piece of the response is received, it immediately prints it out. The setup makes sure there are no new lines between parts, so it all looks like one continuous response.\n",
    "\n",
    "3. **No Waiting**: By using this loop, you don't have to wait for the entire response to be ready before you start seeing parts of it. This makes it feel quicker and more like a conversation.\n",
    "\n",
    "This way, the loop helps provide a smoother and more interactive way of displaying responses from the system as they are generated.\n",
    "\n",
    "#### The for loop explained in technical terms\n",
    "This `for` loop is used to handle streaming output from a language model response. Hereâ€™s a breakdown of its functionality and context:\n",
    "\n",
    "1. **Iteration through Streamed Output**: The `for` loop iterates over the generator returned by `chain.stream(...)`. The `stream` method of the `chain` object (which is a combination of a prompt template and a language model) is designed to yield responses incrementally. This is particularly useful when responses from a model are long or need to be displayed in real-time.\n",
    "\n",
    "2. **Data Fetching**: In this loop, `s` represents each piece of content that is streamed back from the model as the response is being generated. The model in this case is set up to respond with information about the politician \"F.D. Roosevelt\".\n",
    "\n",
    "3. **Output Handling**: Inside the loop, `print(s.content, end=\"\", flush=True)` is called for each piece of streamed content. The `print` function is customized with:\n",
    "   - `end=\"\"`: This parameter ensures that each piece of content is printed without adding a new line after each one, thus allowing the response to appear continuous on a single line.\n",
    "   - `flush=True`: This parameter forces the output buffer to be flushed immediately after each print statement, ensuring that each piece of content is displayed to the user as soon as it is received without any delay.\n",
    "\n",
    "By using this loop, the code is able to display each segment of the model's response as it becomes available, making the user interface more dynamic and responsive, especially for real-time applications where prompt feedback is beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b680921b-e75f-4d77-b655-22dc9a91e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Lenin was a Russian revolutionary and politician who led the Bolshevik party to power during the October Revolution of 1917.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 14, 'total_tokens': 38}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0d5ae3bf-268e-4c7b-bae0-53991c88bdc9-0', usage_metadata={'input_tokens': 14, 'output_tokens': 24, 'total_tokens': 38}),\n",
       " AIMessage(content='Stalin was a ruthless dictator responsible for millions of deaths in the Soviet Union.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 14, 'total_tokens': 30}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b5e99220-7665-49a4-a124-653027e78b8f-0', usage_metadata={'input_tokens': 14, 'output_tokens': 16, 'total_tokens': 30})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"politician\": \"Lenin\"}, {\"politician\": \"Stalin\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0a455-5f6b-4b32-8cef-4d6ccce13526",
   "metadata": {},
   "source": [
    "#### LCEL Chains/Runnables can be also used asynchronously:\n",
    "* chain.ainvoke(): call the chain on an input.\n",
    "* chain.astream(): call the chain on an input and stream back chunks of the response.\n",
    "* chain.abatch(): call the chain on a list of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e97ca-f124-4c0d-9565-2f60a1b92d49",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 004-invoke-stream-batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaa8ab-caa4-48a5-9a12-c14881889bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
