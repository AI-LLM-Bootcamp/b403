{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Main built-in LCEL Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46ff75-72ab-4391-904f-459bae77fc7a",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* RunnablePassthrough.\n",
    "* RunnableLambda.\n",
    "* RunnableParallel.\n",
    "    * itemgetter.\n",
    "* RunnableBranch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 005-builtin-runnables.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5fbb27-418c-4938-a82f-a40a01cba0ee",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **005-builtin-runnables**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97622d-b4bb-4ea2-9bc3-9b72beaae588",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008faa2-7f0f-45a5-8cd7-8e73114fdc35",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3f063-722c-4716-b794-4600a066e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0e9eb-1c55-4cee-ba20-be37a5a80c01",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd6f1e-3a7f-4695-afd5-e3dd57f13f86",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe937d-dce4-43e7-a647-7a86dbd37923",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "* It does not do anything to the input data.\n",
    "* Let's see it in a very simple example: a chain with just RunnablePassthrough() will output the original input without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb318d05-70ee-4f57-81a3-9a512f10bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a4b9de-a4b3-4687-82b7-dcfe910f23de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abram'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94758c3-95fc-465b-aec1-8048ab161d45",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "* To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda.\n",
    "* Let's define a very simple function to create Russian lastnames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3438c0-05c3-48ab-a1eb-60209717cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c8c84-94b5-4fcc-910b-915db65bc9a7",
   "metadata": {},
   "source": [
    "* In order to use this function inside of a LCEL chain, we will use RunnableLambda():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bbdd15-32b8-4594-9395-a33c74e95fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e9bdaa7-0cc8-49f8-9aeb-b0ddece0d55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abramovich'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b8f10-0138-432f-b694-8a5360422221",
   "metadata": {},
   "source": [
    "* As you see, our chain is now applying the russian_lastname function to our input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95190509-301b-4be2-9b8b-7dc17193bfee",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "* We will use RunnableParallel() for running tasks in parallel.\n",
    "* This is probably the most important and most useful Runnable from LangChain.\n",
    "* In the following chain, RunnableParallel is going to run these two tasks in parallel:\n",
    "    * operation_a will use RunnablePassthrough.\n",
    "    * operation_b will use RunnableLambda with the russian_lastname function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c6bef0b-399d-43b8-9455-b7f2a9050172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ac0a07-6bec-4cc3-9049-b1ea49d560b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'Abram', 'operation_b': 'Abramovich'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Abram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5372a0-a33e-4a02-9f84-43292bcde994",
   "metadata": {},
   "source": [
    "* Instead of using RunnableLambda, now we are going to use a lambda function and we will invoke the chain with two inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "971fa8a7-5866-4716-9c4f-67260283c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": lambda x: x[\"name\"]+\"ovich\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b45b1a-6352-4618-a5e3-6cbda7236404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': {'name1': 'Jordam', 'name': 'Abram'},\n",
       " 'soccer_player': 'Abramovich'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3cd68-c918-4b4d-9769-bbaee8adb208",
   "metadata": {},
   "source": [
    "* See how the lambda function is taking the \"name\" input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7891ee-c48c-4a26-8b24-1465a43f774e",
   "metadata": {},
   "source": [
    "#### We can add more Runnables to the chain\n",
    "* In the following example, the prompt Runnable will take the output of the RunnableParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f786101f-a555-445f-8c26-29d9b65a37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dece6a57-ef92-4ca8-af7c-cfbd0b9a03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dea6be27-c67f-4bf0-a3db-99d716778486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a234758-44f5-4bdc-a1ea-ff972e23ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "        \"operation_c\": RunnablePassthrough(),\n",
    "    }\n",
    ") | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83889244-92be-4d35-a439-82c0949ccdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Roman Abramovich is that he has a personal net worth estimated to be around $14 billion, making him one of the richest individuals in Russia. He made his fortune primarily through investments in oil and gas companies, as well as through his ownership of the Chelsea Football Club in the English Premier League. Despite his immense wealth, Abramovich is known for his philanthropic efforts, having donated millions of dollars to various charities and causes over the years.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1f143-f7eb-4f86-9f49-e4dc8c249451",
   "metadata": {},
   "source": [
    "* As you saw, the prompt Runnable took \"Abramovich\", the output of the RunnableParallel, as the value for the \"soccer_player\" variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f9c36-ff04-4ac2-9794-503b021018dd",
   "metadata": {},
   "source": [
    "## Let's see a more advanced use of RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf504a95-c3d3-4fd5-a0b1-58d4618cb8fc",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following packages because they are already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89820004-06e2-47f9-a53a-52d9753286ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9048e7b-1e71-4cbc-bb7a-2c66543d6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "999c0201-35df-4ebf-b46e-be0ef40f672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Alumni of AI Accelera are individuals from all continents and top companies.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 10,000 Alumni from all continents and top companies\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"who are the Alumni of AI Accelera?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9fdb9-f85f-4b14-9cd4-7cf3f841295b",
   "metadata": {},
   "source": [
    "#### Important: the syntax of RunnableParallel can have several variations.\n",
    "* When composing a RunnableParallel with another Runnable you do not need to wrap it up in the RunnableParallel class. Inside a chain, the next three syntaxs are equivalent:\n",
    "    * `RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})`\n",
    "    * `RunnableParallel(context=retriever, question=RunnablePassthrough())`\n",
    "    * `{\"context\": retriever, \"question\": RunnablePassthrough()}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010288fe-3891-4d9d-952b-c9c31c27e95c",
   "metadata": {},
   "source": [
    "## Using itemgetter with RunnableParallel\n",
    "* When you are calling the LLM with several different input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1d721b7-6208-47c4-8a51-0104329a849d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Accelera has trained more than 3,000 Enterprise Alumni. Arrr!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 5,000 Enterprise Alumni.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many Enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50a4a2-8a4a-4f6a-ab4b-ac45d3cdb228",
   "metadata": {},
   "source": [
    "## RunnableBranch: Router Chain\n",
    "* A RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input.\n",
    "* **A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable**. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
    "* For advanced uses, a [custom function](https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/) may be a better alternative than RunnableBranch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0108154-e304-4aa2-aa97-f64529a81ade",
   "metadata": {},
   "source": [
    "The following advanced example can classify and respond to user questions based on specific topics like rock, politics, history, sports, or general inquiries. **It uses some new topics that we will explain in the following lesson**. Here’s a simplified explanation of each part:\n",
    "\n",
    "1. **Prompt Templates**: Each template is tailored for a specific topic:\n",
    "   - **rock_template**: Configured for rock and roll related questions.\n",
    "   - **politics_template**: Tailored to answer questions about politics.\n",
    "   - **history_template**: Designed for queries related to history.\n",
    "   - **sports_template**: Set up to address sports-related questions.\n",
    "   - **general_prompt**: A general template for queries that don't fit the specific categories.\n",
    "\n",
    "   Each template includes a placeholder `{input}` where the actual user question will be inserted.\n",
    "\n",
    "2. **RunnableBranch**: This is a branching mechanism that selects which template to use based on the topic of the question. It evaluates conditions (like `x[\"topic\"] == \"rock\"`) to determine the topic and uses the appropriate prompt template.\n",
    "\n",
    "3. **Topic Classifier**: A Pydantic class that classifies the topic of a user's question into one of the predefined categories (rock, politics, history, sports, or general).\n",
    "\n",
    "4. **Classifier Chain**:\n",
    "   - **Chain**: Processes the user's input to predict the topic.\n",
    "   - **Parser**: Extracts the predicted topic from the classifier's output.\n",
    "\n",
    "5. **RunnablePassthrough**: This component feeds the user's input and the classified topic into the RunnableBranch.\n",
    "\n",
    "6. **Final Chain**:\n",
    "   - The user's input is first processed to classify its topic.\n",
    "   - The appropriate prompt is then selected based on the classified topic.\n",
    "   - The selected prompt is used to formulate a question which is then sent to a model (like ChatOpenAI).\n",
    "   - The model’s response is parsed as a string and returned.\n",
    "\n",
    "7. **Execution**:\n",
    "   - The chain is invoked with a sample question, \"Who was Napoleon Bonaparte?\" \n",
    "   - Based on the classification, it selects the appropriate template, generates a query to the chat model, and processes the response.\n",
    "\n",
    "The system effectively creates a dynamic response generator that adjusts the way it answers based on the topic of the inquiry, making use of specialized knowledge for different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d269f55-2a46-4855-b5bc-095027386c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "rock_template = \"\"\"You are a very smart rock and roll professor. \\\n",
    "You are great at answering questions about rock and roll in a concise\\\n",
    "and easy to understand manner.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "rock_prompt = PromptTemplate.from_template(rock_template)\n",
    "\n",
    "politics_template = \"\"\"You are a very good politics professor. \\\n",
    "You are great at answering politics questions..\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "politics_prompt = PromptTemplate.from_template(politics_template)\n",
    "\n",
    "history_template = \"\"\"You are a very good history teacher. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_prompt = PromptTemplate.from_template(history_template)\n",
    "\n",
    "sports_template = \"\"\" You are a sports teacher.\\\n",
    "You are great at answering sports questions.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "sports_prompt = PromptTemplate.from_template(sports_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083a779-e9fe-4dbf-9e0b-ead57ddfa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa09888-ee51-4f43-91c5-0f3136ce385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"rock\", rock_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"politics\", politics_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"history\", history_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"sports\", sports_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b47e6-e3f4-4374-8f20-c0a884377d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    \n",
    "    topic: Literal[\"rock\", \"politics\", \"history\", \"sports\"]\n",
    "    \"The topic of the user question. One of 'rock', 'politics', 'history', 'sports' or 'general'.\"\n",
    "\n",
    "classifier_function = convert_to_openai_function(TopicClassifier)\n",
    "\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "\n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f825ac1-df8b-4a6d-a24d-036b4c758e7b",
   "metadata": {},
   "source": [
    "The `classifier_function` classifies or categorizes the topic of a user's question into specific categories such as \"rock,\" \"politics,\" \"history,\" or \"sports.\" Here’s how it works in simple terms:\n",
    "\n",
    "1. **Conversion to Function**: It converts the `TopicClassifier` Pydantic class, which is a predefined classification system, into a function that can be easily used with LangChain. This conversion process involves wrapping the class so that it can be integrated and executed within an OpenAI model.\n",
    "\n",
    "2. **Topic Detection**: When you input a question, this function analyzes the content of the question to determine which category or topic it belongs to. It looks for keywords or patterns that match specific topics. For example, if the question is about a rock band, the classifier would identify the topic as \"rock.\"\n",
    "\n",
    "3. **Output**: The function outputs the identified topic as a simple label, like \"rock\" or \"history.\" This label is then used by other parts of the LangChain to decide how to handle the question, such as choosing the right template for formulating a response.\n",
    "\n",
    "In essence, the `classifier_function` acts as a smart filter that helps the system understand what kind of question is being asked so that it can respond more accurately and relevantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57438410-02c8-4808-abeb-1f941a7fafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain) \n",
    "    | prompt_branch \n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf2559-318a-4d84-ae14-6582232b19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain.invoke(\n",
    "    {\"input\": \"Who was Napoleon Bonaparte?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521a3fe-c9a0-4e29-aad4-027fb6659d98",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 005-builtin-runnables.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 005-builtin-runnables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab9741-4f52-4220-ac4d-ff37035cddbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
